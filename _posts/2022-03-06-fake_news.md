---
layout: post
title: Fakeout? News Classification with Tensorflow
---

In this blog post, we will be using tensorflow to build a Fake news classifier. With the rampant spread of fake news on the modern web and the threat it poses to democracy arround the world, this is both and interesting and topical problem. We will be using the keras functional API to build three different models. One will use use the title of the article to make the prediction, one will be using just the article text, and one will use both. We will then compare which model produces the most accurate output. Lets start with our standard imports and then get going building this model.


```python
import pandas as pd
import numpy as np 
import tensorflow as tf
import re
import string
from matplotlib import pyplot as plt

from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers.experimental.preprocessing import StringLookup

from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow import keras
```

Now lets import the training data and read it into a csv file for easy access.


```python
#import the training url
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
```


```python
#read in the data from the csv file
train_data=pd.read_csv(train_url)
```


```python
#examine the data
train_data.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and â€œClose Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>



As we can see the data has three columns of interest. One represents the article title, one represents the article text and one is classification of real or fake news. Next we'll write a function which takes in a dataset in the form of a pandas dataframe, removes the stopwords from the title and text columns, and returns the data as a tensorflow Dataset.


```python
from sklearn.feature_extraction import text

def make_dataset(data):
    ''' 
    This function takes in a dataset in the form of a pandas dataframe, removes the 
    stopwords from the title and text columns, and returns the data as a tensorflow 
    Dataset.
    
    @params
    data-pd: pandas dataframe with title, text, and fake columns
    
    @return 
    Data_set- Tensorflow Dataset with title, text as imputs and fake as output
    '''
    #download the list of stopwords from sklearn
    stop = text.ENGLISH_STOP_WORDS
    
    #remove stopwords from the title column
    data['title']=data['title'].apply(lambda x: ' '.join(
        [word for word in x.split() if word not in (stop)]))
    
    #remove stopwords from the text column
    data['text']=data['text'].apply(lambda x: ' '.join(
        [word for word in x.split() if word not in (stop)]))
    
    #turn the pandas dataframe into a tensorflow dataset with title and text as
    #input and fake as output
    Data_set=tf.data.Dataset.from_tensor_slices(
    (
        {
            "title" : data[["title"]], 
            "text" : data[["text"]]
        }, 
        {
            "fake" : data[["fake"]]
        }
    )
)
    Data_set.batch(100)
    return Data_set

```

Now lets use that function to clean our fake news dataset and then seperate the dataset into training and validation sets. We will then calculate the number of fake and real news articles in our dataset in order to figure out the accuracy of the baseline model (the model that always choses the most common label).


```python
#make a cleaned dataset using make dataset function defined above
Cleaned_dataset=make_dataset(train_data)
Cleaned_dataset
```




    <TensorSliceDataset element_spec=({'title': TensorSpec(shape=(1,), dtype=tf.string, name=None), 'text': TensorSpec(shape=(1,), dtype=tf.string, name=None)}, {'fake': TensorSpec(shape=(1,), dtype=tf.int64, name=None)})>




```python
#define the size of the training set
train_size=int(0.8*len(Cleaned_dataset))

#create train and validate sets based on the training size
train=Cleaned_dataset.take(train_size)
validate=Cleaned_dataset.skip(train_size)
```


```python
#calculte the number of fake and real article in the dataset
total_number = train_data['fake'].size
fake_number = train_data['fake'].sum()
true_number=total_number-fake_number
```


```python
for data, fake in Cleaned_dataset.take(1):
    print(fake)
```

    {'fake': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>}



```python
fake_number
```




    11740




```python
true_number
```




    10709




```python
total_number
```




    22449



## Model 1-Using Title

We're ready to build our first model! This model will use only the title of the article as the predictor for the classification. First lets define a standardization function which takes in our input data and cleans it, removing punctionand making all letters lowercase for greater interpretability by our model, now we will use this in our vectorization layer. Next we will create a vectorization layer in order to turn text input into numbers to make it understandable to our model. We will addapt the vectorization layer to our specific data, this means that the string-number conversions the layer makes will be tuned to the words most common in our dataset. 


```python
#define a standardization function
def standardization(input_data):
    #make the strings lowercase
    lowercase = tf.strings.lower(input_data)
    #get rid of punctuation
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation 
```


```python
#create a vectorizor layer
vectorize_layer= TextVectorization(
    standardize=standardization,
    max_tokens=2000,
    output_mode='int',
    output_sequence_length=25) 

#addapt the vectorizor layer to the training data
vectorize_layer.adapt(train.map(lambda x, y: x["title"]))
```

We're now ready to define our input using the keras.input function. Once input is defined, were ready to build our model. We will use the keras function api to build a graph of layers. First we add our predefined and addapted vectorization layer, then an embedding layer to help the model make sense of the words it is being fed. After this, we add dropout and average pooling layers to avoid overfitting and create more abstraction before finally running the output through a few more dense layers to add more abstraction and finally spit out a binary classification. We use this defined graph of layers along with the input we defined above to create our model.


```python
num_chars=vectorize_layer.vocabulary_size()

#define the title input
title_input=keras.Input(
    shape = (1,),
    name='title',
    dtype='string'
)
#define the layers of the model 
title_features = vectorize_layer(title_input)
title_features = layers.Embedding(num_chars, 10, name='embedding')(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(32, activation='relu')(title_features)
title_features = layers.Dense(32, activation='relu')(title_features)

title_features = layers.Dropout(0.2)(title_features)
output = layers.Dense((2), name='fake')(title_features)
```


```python
#build the model
model1=keras.Model(
    inputs=[title_input],
    outputs=output
)

```


```python
model1.summary()
```

    Model: "model_6"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     title (InputLayer)          [(None, 1)]               0         
                                                                     
     text_vectorization_2 (TextV  (None, 25)               0         
     ectorization)                                                   
                                                                     
     embedding (Embedding)       (None, 25, 10)            20000     
                                                                     
     dropout_26 (Dropout)        (None, 25, 10)            0         
                                                                     
     global_average_pooling1d_10  (None, 10)               0         
      (GlobalAveragePooling1D)                                       
                                                                     
     dropout_27 (Dropout)        (None, 10)                0         
                                                                     
     dense_22 (Dense)            (None, 32)                352       
                                                                     
     dense_23 (Dense)            (None, 32)                1056      
                                                                     
     dropout_28 (Dropout)        (None, 32)                0         
                                                                     
     fake (Dense)                (None, 2)                 66        
                                                                     
    =================================================================
    Total params: 21,474
    Trainable params: 21,474
    Non-trainable params: 0
    _________________________________________________________________



```python
model1(tf.constant([['sample string']]))
```




    <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.05409827,  0.00154433]], dtype=float32)>



We are now ready to train and compile the model.



```python
#compile the model
model1.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
#train the model and save the train data to the history variable
history1 = model1.fit(train, 
                    validation_data=validate,
                    epochs = 5, 
                    verbose = True)
```

    Epoch 1/5


    /Users/conormccaulley/opt/anaconda3/lib/python3.8/site-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.
      inputs = self._flatten_to_reference_inputs(inputs)


    17959/17959 [==============================] - 44s 2ms/step - loss: 0.0901 - accuracy: 0.9679 - val_loss: 0.0496 - val_accuracy: 0.9822
    Epoch 2/5
    17959/17959 [==============================] - 39s 2ms/step - loss: 0.0503 - accuracy: 0.9830 - val_loss: 0.0520 - val_accuracy: 0.9822
    Epoch 3/5
    17959/17959 [==============================] - 41s 2ms/step - loss: 0.0405 - accuracy: 0.9860 - val_loss: 0.0553 - val_accuracy: 0.9808
    Epoch 4/5
    17959/17959 [==============================] - 41s 2ms/step - loss: 0.0358 - accuracy: 0.9881 - val_loss: 0.0601 - val_accuracy: 0.9797
    Epoch 5/5
    17959/17959 [==============================] - 46s 3ms/step - loss: 0.0329 - accuracy: 0.9890 - val_loss: 0.0628 - val_accuracy: 0.9802



```python
#plot the training history
plt.plot(history1.history["accuracy"], label = "training")
plt.plot(history1.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7fc83545e820>




![output_29_1.png]({{ site.baseurl }}/images/output_29_1.png)    

    


After training we're ready to evaluate our model! As you can see we acheived ~98% accuracy, not bad for just using the article title.


```python
model1.evaluate(validate)
```

    4490/4490 [==============================] - 5s 1ms/step - loss: 0.0628 - accuracy: 0.9802





    [0.06284219026565552, 0.98017817735672]



## Model 2-Using text

This next model is almost exactly the same as the one above except we define a new input that takes the text rather than the title as the input data. All the other steps are identicle.


```python
vectorize_layer.adapt(train.map(lambda x, y: x["text"]))
```


```python
#define the title input
text_input=keras.Input(
    shape = (1,),
    name='text',
    dtype='string'
)
#define the layers of the model 
text_features = vectorize_layer(text_input)
text_features = layers.Embedding(num_chars, 10, name='embedding')(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(32, activation='relu')(text_features)
text_features = layers.Dense(32, activation='relu')(text_features)

text_features = layers.Dropout(0.2)(text_features)
output2 = layers.Dense((2), name='fake')(text_features)
```


```python
model2=keras.Model(
    inputs=[text_input],
    outputs=output2
)



#compile the model
model2.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
history2 = model2.fit(train, 
                    validation_data=validate,
                    epochs = 5, 
                    verbose = True)
```

    Epoch 1/5


    /Users/conormccaulley/opt/anaconda3/lib/python3.8/site-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.
      inputs = self._flatten_to_reference_inputs(inputs)


    17959/17959 [==============================] - 34s 2ms/step - loss: 0.1857 - accuracy: 0.9258 - val_loss: 0.1334 - val_accuracy: 0.9483
    Epoch 2/5
    17959/17959 [==============================] - 35s 2ms/step - loss: 0.1347 - accuracy: 0.9512 - val_loss: 0.1331 - val_accuracy: 0.9463
    Epoch 3/5
    17959/17959 [==============================] - 28s 2ms/step - loss: 0.1218 - accuracy: 0.9569 - val_loss: 0.1348 - val_accuracy: 0.9488
    Epoch 4/5
    17959/17959 [==============================] - 29s 2ms/step - loss: 0.1154 - accuracy: 0.9584 - val_loss: 0.1314 - val_accuracy: 0.9492
    Epoch 5/5
    17959/17959 [==============================] - 29s 2ms/step - loss: 0.1117 - accuracy: 0.9614 - val_loss: 0.1304 - val_accuracy: 0.9490



```python
#plot the training history
plt.plot(history2.history["accuracy"], label = "training")
plt.plot(history2.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7fc8355e8b50>




![output_38_1.png]({{ site.baseurl }}/images/output_38_1.png)      
    


Evaluating this model, we can see that it acheives ~95% accuracy, a little lower than the last model but still quite good.


```python
model2.evaluate(validate)
```

    4490/4490 [==============================] - 4s 975us/step - loss: 0.1304 - accuracy: 0.9490





    [0.13041065633296967, 0.9489977955818176]



## Model 3- Using title and text

Our final model uses both the title and text of the article and is basically the 2 models above smashed together. We acheive this by defining text input and title input as above, defining the text feature layer graph and title feature layer graph as above, and then concatinating the 2 layer graphs together using layers.concatenate. Note the the same embedding layer is used in both input flows. The output from that concatenation then gets passed through one final dense layer to create the binary classification output.


```python
text_input=keras.Input(
    shape = (1,),
    name='text',
    dtype='string'
)

#define the title input
title_input=keras.Input(
    shape = (1,),
    name='title',
    dtype='string'
)

embedding=layers.Embedding(num_chars, 10, name='embedding')

#define the layers of the model 
title_features = vectorize_layer(title_input)
title_features = embedding(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(32, activation='relu')(title_features)
title_features = layers.Dense(32, activation='relu')(title_features)


text_features = vectorize_layer(text_input)
text_features = embedding(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(32, activation='relu')(text_features)
text_features = layers.Dense(32, activation='relu')(text_features)


main = layers.concatenate([title_features, text_features], axis = 1)
main = layers.Dense(32, activation='relu')(main)


output3 = layers.Dense((2), name='fake')(main)
```


```python
#create the model from the inputs and layer graph
model3 = keras.Model(
    inputs = [title_input, text_input],
    outputs = output3
)

#compile the model
model3.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
model3.summary()
```

    Model: "model_5"
    __________________________________________________________________________________________________
     Layer (type)                   Output Shape         Param #     Connected to                     
    ==================================================================================================
     title (InputLayer)             [(None, 1)]          0           []                               
                                                                                                      
     text (InputLayer)              [(None, 1)]          0           []                               
                                                                                                      
     text_vectorization_2 (TextVect  (None, 25)          0           ['title[0][0]',                  
     orization)                                                       'text[0][0]']                   
                                                                                                      
     embedding (Embedding)          (None, 25, 10)       20000       ['text_vectorization_2[7][0]',   
                                                                      'text_vectorization_2[8][0]']   
                                                                                                      
     dropout_22 (Dropout)           (None, 25, 10)       0           ['embedding[0][0]']              
                                                                                                      
     dropout_24 (Dropout)           (None, 25, 10)       0           ['embedding[1][0]']              
                                                                                                      
     global_average_pooling1d_8 (Gl  (None, 10)          0           ['dropout_22[0][0]']             
     obalAveragePooling1D)                                                                            
                                                                                                      
     global_average_pooling1d_9 (Gl  (None, 10)          0           ['dropout_24[0][0]']             
     obalAveragePooling1D)                                                                            
                                                                                                      
     dropout_23 (Dropout)           (None, 10)           0           ['global_average_pooling1d_8[0][0
                                                                     ]']                              
                                                                                                      
     dropout_25 (Dropout)           (None, 10)           0           ['global_average_pooling1d_9[0][0
                                                                     ]']                              
                                                                                                      
     dense_17 (Dense)               (None, 32)           352         ['dropout_23[0][0]']             
                                                                                                      
     dense_19 (Dense)               (None, 32)           352         ['dropout_25[0][0]']             
                                                                                                      
     dense_18 (Dense)               (None, 32)           1056        ['dense_17[0][0]']               
                                                                                                      
     dense_20 (Dense)               (None, 32)           1056        ['dense_19[0][0]']               
                                                                                                      
     concatenate_1 (Concatenate)    (None, 64)           0           ['dense_18[0][0]',               
                                                                      'dense_20[0][0]']               
                                                                                                      
     dense_21 (Dense)               (None, 32)           2080        ['concatenate_1[0][0]']          
                                                                                                      
     fake (Dense)                   (None, 2)            66          ['dense_21[0][0]']               
                                                                                                      
    ==================================================================================================
    Total params: 24,962
    Trainable params: 24,962
    Non-trainable params: 0
    __________________________________________________________________________________________________



```python
#train the model and save the train data to the history variable
history3 = model3.fit(train, 
                    validation_data=validate,
                    epochs = 5, 
                    verbose = True)
```

    Epoch 1/5
    17959/17959 [==============================] - 46s 2ms/step - loss: 0.0728 - accuracy: 0.9717 - val_loss: 0.0256 - val_accuracy: 0.9909
    Epoch 2/5
    17959/17959 [==============================] - 46s 3ms/step - loss: 0.0292 - accuracy: 0.9891 - val_loss: 0.0203 - val_accuracy: 0.9933
    Epoch 3/5
    17959/17959 [==============================] - 37s 2ms/step - loss: 0.0198 - accuracy: 0.9929 - val_loss: 0.0207 - val_accuracy: 0.9933
    Epoch 4/5
    17959/17959 [==============================] - 44s 2ms/step - loss: 0.0142 - accuracy: 0.9951 - val_loss: 0.0324 - val_accuracy: 0.9929
    Epoch 5/5
    17959/17959 [==============================] - 38s 2ms/step - loss: 0.0154 - accuracy: 0.9954 - val_loss: 0.0240 - val_accuracy: 0.9935



```python
#plot the training history
plt.plot(history3.history["accuracy"], label = "training")
plt.plot(history3.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7fc83581cfd0>




![output_47_1.png]({{ site.baseurl }}/images/output_47_1.png)      




```python
#evaluate the model on the validation data
model3.evaluate(validate)
```

    4490/4490 [==============================] - 6s 1ms/step - loss: 0.0240 - accuracy: 0.9935





    [0.024032894521951675, 0.9935411810874939]



Of the three models, the model using both the title and the text was able to acheive the highest validation accuracy at over 99%. This makes sense since both the title and the text may contain certain indicators of a an article being fake. Based on this, algorithms made to detect fake news should take both title and text into account.

## Testing on unseen data

Lets now test on a previously unseen dataset.


```python
#read the data to a csv file
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
test_data=pd.read_csv(test_url)
```


```python
#clean the dataset using our make_dataset function
Cleaned_test_dataset=make_dataset(test_data)
Cleaned_test_dataset
```




    <TensorSliceDataset element_spec=({'title': TensorSpec(shape=(1,), dtype=tf.string, name=None), 'text': TensorSpec(shape=(1,), dtype=tf.string, name=None)}, {'fake': TensorSpec(shape=(1,), dtype=tf.int64, name=None)})>




```python
#evaluate the model on the unseen data.
model3.evaluate(Cleaned_test_dataset)
```

    22449/22449 [==============================] - 30s 1ms/step - loss: 0.0411 - accuracy: 0.9907





    [0.041077252477407455, 0.9907345771789551]



The model which incorporated both title and text was able to obtain ~99% accuracy on the unseen test dataset. Based on this finding, we could expect this model to detect fake news ~99% of the time though I suspect that subtler fake news that is written in a more professional way might slip through the cracks since the model is trained to find paterns in how the articles are written but can't fact check individual statements. 

## Visualizing the Embedding Layer

With our models completed, we have just one more thing to do, visualize the word associations created by the models embedding layer. We'll do this by getting the weights from the embedding layer, using principle component analysis to reduce the dimensions to 2d and then plotting this reduced 2d version in a scatterplot so we can visualize any interesting trends.


```python
len(vocab)
len(weights)
```




    2000




```python
#retreive weights and vobal from our vectorizer and embedding layer
weights = model3.get_layer('embedding').get_weights()[0] # get the weights from the embedding layer
vocab = vectorize_layer.get_vocabulary()                # get the vocabulary from our data prep for later

#use PCA to reduce the 10 dimmentinal word embedding to 2d
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
weights = pca.fit_transform(weights)

#create a pandas dataframe to plot
embedding_df = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})

#plot the dataframe we created above
import plotly.express as px 
fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                
                 size = list(np.ones(len(embedding_df))),
                 size_max = 2,
                 hover_name = "word")

fig.show()
```
![embbed.png]({{ site.baseurl }}/images/embbed.png) 



There are a number of interesting thing that we can note about the above embedding plot:
1. Looking in the top right hand corner we see a tight clump containing the words 'germanys', 'japans', 'chinas', 'zimbabwes', 'brazils', and 'canadas'. I found this to be particularly interesting since the model was clearly able to learn the concept of a word which represents 'belonging' to a certain country and cluster them together. 
2. One of the points most corrilated to fame news on the top, far left side was 'mainstream' likely in reference to the 'mainstream' media. This makes a lot of sense since far right news site tend to spend a lot of time attacking what they call 'mainstream media which is really anything outside of the far right.
3. The model was also able to learn the concept of 'belonging to a particlar person as it was able to clump 'obamas' and 'trumps' together in the botton left hand corner. 
4. 'Rightwing', on the mid right side of the model, tended to be more associated with real news which again makes sense since it is fairly unlikely that far right propagandists would refer to themselfs and 'rightwing'. You tend to see this used more commonly in reference to far right groups from more centrist and left leaning news sources. 
5. 'pnony' and 'hollywood' were also clusterd together and associated with fake news articles. This makes sense given the propensity of far right news outlets that often peddle fake news to attack the 'phonys in hollywood who control the media'. 


```python

```
